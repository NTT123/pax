{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.7.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.3 64-bit"
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "colab": {
      "name": "Pax's Performance.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## The performance cost of using Pax\n",
        "\n",
        "Even though `jax.jit` can eliminate almost all performance penalties related to Pax, there is a small cost of calling `tree_flatten` and `tree_unflatten` for the inputs and outputs of a jitted function.\n",
        "\n",
        "In this tutorial, we will measure the performance cost of using Pax. We also introduce practices that help to reduce the cost.\n",
        "\n",
        "**Note**: This cost is usually less than 1% of the training time. Most of the time, we can ignore it."
      ],
      "metadata": {
        "id": "vhmqzGvAm80Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us start with a simple code for training a ResNet50 classifier."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pax, jax, opax\n",
        "import jax.numpy as jnp\n",
        "from pax.nets import ResNet50\n",
        "\n",
        "def loss_fn(params: ResNet50, model: ResNet50, inputs):\n",
        "    images, labels = inputs\n",
        "    model = model.update(params)\n",
        "    log_pr = jax.nn.log_softmax(model(images), axis=-1)\n",
        "    loss = jnp.mean(jnp.sum(jax.nn.one_hot(labels, num_classes=10) * log_pr, axis=-1))\n",
        "    return loss, (loss, model)\n",
        "\n",
        "@pax.jit\n",
        "def update(model: ResNet50, optimizer: opax.GradientTransformation, inputs):\n",
        "    grads, (loss, model) = pax.grad(loss_fn, has_aux=True)(model.parameters(), model, inputs)\n",
        "    model = model.update(\n",
        "        optimizer.step(grads, model.parameters()),\n",
        "    )\n",
        "    return loss, model, optimizer\n",
        "\n",
        "net = ResNet50(3, 10)\n",
        "optimizer = opax.adam(1e-4)(net.parameters())\n",
        "\n",
        "rng_key = jax.random.PRNGKey(42)\n",
        "img = jax.random.normal(rng_key,  (1, 3, 64, 64))\n",
        "label = jax.random.randint(rng_key, (1,), 0, 10)\n",
        "# loss, net, optimizer = update(net, optimizer, (img, label))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Seeding RNG key with seed 42. Use `pax.seed_rng_key` function to avoid this warning.\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jixDaQMym80j",
        "outputId": "f4f8a766-b061-46d8-ab3a-7cb6530a6a64"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import time \n",
        "start = time.perf_counter()\n",
        "for i in range(10_000):\n",
        "    a, b = jax.tree_flatten((net, optimizer))\n",
        "    (net, optimizer) = jax.tree_unflatten(b, a)\n",
        "end = time.perf_counter()\n",
        "print(\"Duration:\", end-start)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 39.52964387499999\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J1bqP1em80r",
        "outputId": "8656877b-9d01-4796-afb9-8163f48aefeb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It takes 39 seconds to execute 10,000 iterations of `tree_flatten` and `tree_unflatten` for  the tuple `(net, optimizer)`.\n",
        "\n",
        "This is the cost of using Pax with a ResNet50 network and an `opax.adam` optimizer."
      ],
      "metadata": {
        "id": "jnZNeI2qpC60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flatten optimizer\n",
        "\n",
        "One easy way to reduce the cost is to use the `flatten` mode supported by `opax` optimizers."
      ],
      "metadata": {
        "id": "h1evjLQrqHDe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "optimizer = opax.adam(1e-4)(net.parameters(), flatten=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "CjYQTqugm80s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this mode, the optimizer will automatically flatten the parameters and gradients to a list of leaves instead of dealing with the full tree structure. This reduces the `flatten` and `unflatten` cost of the optimizer to almost zero.\n",
        "\n",
        "However, we are no longer able to access the optimizer's pytree objects. \n",
        "Fortunately, we rarely need to access the optimizer's pytree objects, and one can easily convert the flatten list back to the pytree object using `jax.tree_unflatten` function.\n"
      ],
      "metadata": {
        "id": "-SWWQIbYqb6K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import time \n",
        "start = time.perf_counter()\n",
        "for i in range(10_000):\n",
        "    a, b = jax.tree_flatten((net, optimizer))\n",
        "    (net, optimizer) = jax.tree_unflatten(b, a)\n",
        "end = time.perf_counter()\n",
        "print(\"Duration:\", end-start)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 9.955761487000004\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo4k-MwHm80t",
        "outputId": "4472ed5e-de15-4754-e441-dc6d195fd601"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "start = time.perf_counter()\n",
        "for i in range(10_000):\n",
        "    a, b = jax.tree_flatten(optimizer)\n",
        "    optimizer = jax.tree_unflatten(b, a)\n",
        "end = time.perf_counter()\n",
        "print(\"Duration:\", end-start)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 0.40090021199999626\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0I79hxtn9qe",
        "outputId": "4ea6c885-1000-414c-c912-0e20861269fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With `flatten=True` we reduce the time to only 9.9 seconds. And the time to flatten/unflatten the `optimizer` alone is almost zeros (0.40 seconds)"
      ],
      "metadata": {
        "id": "PPmiJJ7Ks6W0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-step update function\n",
        "\n",
        "Another solution to reduce Pax's cost is to execute multiple update steps inside a jitted function."
      ],
      "metadata": {
        "id": "i5ttrXP2tRTH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "num_steps = 10\n",
        "\n",
        "@pax.jit\n",
        "def multistep_update(model, optimizer, inputs):\n",
        "    def scan_loop(prev, batch):\n",
        "        model, optimizer = prev\n",
        "        loss, model, optimizer = update(model, optimizer, batch)\n",
        "        return (model, optimizer), loss\n",
        "    \n",
        "    (model, optimizer), loss = pax.utils.scan(scan_loop, (model, optimizer), inputs)\n",
        "    loss = jnp.array(0.)\n",
        "    return loss, model, optimizer\n",
        "\n",
        "multistep_img = jax.random.normal(rng_key,  (num_steps, 1, 3, 64, 64))\n",
        "multistep_label = jax.random.randint(rng_key, (num_steps, 1,), 0, 10)\n",
        "# loss, net, optimizer = multistep_update(net, optimizer, (multistep_img, multistep_label))"
      ],
      "outputs": [],
      "metadata": {
        "id": "AVdCuvf0m80v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `multistep_update` function will execute multiple update steps in a single call.\n",
        "If `num_steps=10`, we can reduce the Pax's cost by a factor of `10`."
      ],
      "metadata": {
        "id": "ybEmPKTQt03C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**:  This approach of executing multiple update steps inside a jitted function is also very useful for TPU. It reduces the communication cost between CPU host and TPU cores, significantly."
      ],
      "metadata": {
        "id": "hDU0JdBjuPiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flatten model"
      ],
      "metadata": {
        "id": "0FzwKNEN57Al"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have reduced Pax's cost significantly with a little effort. This final solution will reduce to cost to almost zero. However, it has downsides too.\n",
        "\n",
        "The idea is simple: we want to move `flatten` and `unflatten` to the inside of the update function."
      ],
      "metadata": {
        "id": "x9zy1WtMvBvF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from functools import partial\n",
        "\n",
        "@partial(pax.jit, static_argnums=0)\n",
        "def flatten_update(model_def, model_leaves, optimizer: opax.GradientTransformation, inputs):\n",
        "    model = jax.tree_unflatten(model_def, model_leaves)\n",
        "    grads, (loss, model) = pax.grad(loss_fn, has_aux=True)(model.parameters(), model, inputs)\n",
        "    model = model.update(\n",
        "        optimizer.step(grads, model.parameters()),\n",
        "    )\n",
        "    return loss, jax.tree_leaves(model), optimizer"
      ],
      "outputs": [],
      "metadata": {
        "id": "nDvYIwCmvl9r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "net_leaves, net_def = jax.tree_flatten(net)\n",
        "\n",
        "# for i in range(10_000):\n",
        "#     loss, net_leaves, optimizer = flatten_update(net_def, net_leaves, optimizer, (img, label))"
      ],
      "outputs": [],
      "metadata": {
        "id": "bPtPze3kwTzm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "start = time.perf_counter()\n",
        "for i in range(10_000):\n",
        "    a, b = jax.tree_flatten((net_leaves, optimizer))\n",
        "    (net_leaves, optimizer) = jax.tree_unflatten(b, a)\n",
        "end = time.perf_counter()\n",
        "print(\"Duration:\", end-start)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 0.7107760669999976\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XTe1Rq8m806",
        "outputId": "9d46f02f-e296-4e70-e4a8-02c85c5aa56c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have reduced the cost to almost zero. However, we have to recreate the model manually from its leaves and tree_def when needed."
      ],
      "metadata": {
        "id": "W1fVZXRZ1Nbi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "net = jax.tree_unflatten(net_def, net_leaves)"
      ],
      "outputs": [],
      "metadata": {
        "id": "-0VfPeG70UVL"
      }
    }
  ]
}