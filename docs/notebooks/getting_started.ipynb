{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Getting started\n",
    "\n",
    "To begin our journey, we need to import `jax` and `pax`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pax"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start by defining a simple ``Linear`` module."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Linear(pax.Module):\n",
    "    \"\"\"A Linear modules has two real parameters ``weight`` and ``bias``.\"\"\"\n",
    "\n",
    "    weight: jnp.ndarray\n",
    "    bias: jnp.ndarray\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() # it is required to call the parent class.\n",
    "\n",
    "        self.register_parameter('weight', jnp.array(1.0))\n",
    "        self.register_parameter('bias', jnp.array(0.0))\n",
    "    \n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        return self.weight * x + self.bias\n",
    "\n",
    "net = Linear()\n",
    "print(net.summary())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**: we are calling `self.register_parameter` method to register `weight` and `bias` as __trainable parameters__ of ``Linear``.\n",
    "\n",
    "Next, we will create a simple fake dataset and use our `Linear` module to fit the data. Note that, ``a = -3.0`` and ``b=1.3`` are the ground-truth weight and bias."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (3,2)\n",
    "\n",
    "\n",
    "pax.seed_rng_key(42) # seeding pax random key\n",
    "\n",
    "def create_data(a=-3.0, b=1.5):\n",
    "    x = jax.random.uniform(pax.next_rng_key(), (128, 1))\n",
    "    noise = jax.random.normal(pax.next_rng_key(), x.shape) * 0.2\n",
    "    y = a * x + b + noise\n",
    "    plt.scatter(x, y)\n",
    "    plt.grid('on')\n",
    "    plt.legend([\"data\"])\n",
    "    plt.show()\n",
    "    return x, y\n",
    "\n",
    "x, y = create_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now plot the inital _predictions_ of our linear module."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_prediction(net, data):\n",
    "    x, y= data\n",
    "    y_hat = net(x) \n",
    "\n",
    "    plt.scatter(x, y)\n",
    "    plt.scatter(x, y_hat)\n",
    "    plt.legend(['data', 'prediction'])\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.grid('on')\n",
    "    plt.show()\n",
    "\n",
    "plot_prediction(net, (x, y))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To fit ``net`` with our data, we need to define a _mean squared error_ loss function which measures the current prediction errors. Our goal is to minimize the loss function using its gradient function and the _stochastic gradient descent_ algorithm."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from typing import Tuple\n",
    "Batch = Tuple[jnp.ndarray, jnp.ndarray]\n",
    "\n",
    "def mse_loss(params: Linear, model: Linear, inputs: Batch):\n",
    "    model = model.update(params)\n",
    "    x, y = inputs\n",
    "    y_hat = model(x)\n",
    "\n",
    "    # mse\n",
    "    loss = jnp.mean(jnp.square(y - y_hat))\n",
    "    return loss, (loss, model)\n",
    "\n",
    "gradient_fn = jax.grad(mse_loss, has_aux=True)\n",
    "\n",
    "params = net.parameters()\n",
    "grad, (loss, net) = gradient_fn(params, net, (x, y))\n",
    "print(grad.weight, grad.bias, loss)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are few _interesting_ points in ``mse_loss`` function:\n",
    "\n",
    "1. ``params`` is a pytree of trainable parameters. \n",
    "2. ``model.update(params)`` returns a new model that uses ``params`` as its trainable parameters.\n",
    "3. ``mse_loss`` returns ``(loss, model)`` as an auxiliary output.\n",
    "\n",
    "These points are all related to the gradient transformation:\n",
    "\n",
    "```python\n",
    "gradient_fn = jax.grad(mse_loss, has_aux=True)\n",
    "```\n",
    "\n",
    "1. By default, ``jax.grad`` returns a gradient function whose gradients are computed with respect to the first argument of ``mse_loss``.\n",
    "Therefore, we define trainable parameters `params` as a separated argument.\n",
    "2. ``model = model.update(params)`` is to let ``model`` uses ``params`` in its forward computation. That makes the returned output ``loss`` depends on ``params``.\n",
    "3. ``has_aux=True`` informs ``jax.grad`` to return a function whose output includes ``(loss, model)``.\n",
    "\n",
    "__Note__: we return ``model`` in the output of ``mse_loss`` to guarantee that any changes to ``model`` inside the ``mse_loss`` function will be passed to the outside world."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sgd(params: Linear, gradient: Linear, lr: float = 1e-1):\n",
    "    updated_params = jax.tree_map(lambda p, g: p - lr * g, params, gradient)\n",
    "    return updated_params"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The inputs to ``sgd`` function includes the trainable parameters ``params`` and the gradient vector of loss function with respect to ``params``.\n",
    "\n",
    "Apply ``sgd`` iteratively to further improve our model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "losses = []\n",
    "for step in range(500):\n",
    "    grad, (loss, net) = gradient_fn(params, net, (x, y))\n",
    "    params = sgd(params, grad)\n",
    "    losses.append(loss)\n",
    "\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, let plot our prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "net = net.update(params)\n",
    "plot_prediction(net, (x, y))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}