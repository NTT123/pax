{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.7.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.3 64-bit"
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "colab": {
      "name": "Pax's Performance.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Improve performance\n",
        "\n",
        "Even though `jax.jit` can eliminate almost all performance penalties related to Pax, there is a small cost of calling `tree_flatten` and `tree_unflatten` for the inputs and outputs of a jitted function.\n",
        "\n",
        "In this tutorial, we will measure Pax's performance. We also introduce practices that help to improve the performance.\n",
        "\n",
        ".. note::\n",
        "    Pax's performance penalties are usually less than 1% of the training time. Most of the time, we can ignore it."
      ],
      "metadata": {
        "id": "vhmqzGvAm80Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us start with a simple code for training a ResNet50 classifier."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# uncomment the following line to install pax\n",
        "# !pip install -q git+https://github.com/NTT123/pax.git"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import pax, jax, opax\n",
        "import jax.numpy as jnp\n",
        "from pax.nets import ResNet50\n",
        "\n",
        "def loss_fn(model: ResNet50, inputs):\n",
        "    images, labels = inputs\n",
        "    log_pr = jax.nn.log_softmax(model(images), axis=-1)\n",
        "    loss = jnp.mean(jnp.sum(jax.nn.one_hot(labels, num_classes=10) * log_pr, axis=-1))\n",
        "    return loss, (loss, model)\n",
        "\n",
        "@pax.jit\n",
        "def update(model, optimizer, inputs):\n",
        "    grads, (loss, model) = pax.grad(loss_fn, has_aux=True)(model, inputs)\n",
        "    model, optimizer = pax.apply_gradients(model, optimizer, grads=grads)\n",
        "    return model, optimizer, loss\n",
        "\n",
        "net = ResNet50(3, 10)\n",
        "optimizer = opax.adam(1e-4)(net.parameters())\n",
        "\n",
        "rng_key = jax.random.PRNGKey(42)\n",
        "img = jax.random.normal(rng_key,  (1, 3, 64, 64))\n",
        "label = jax.random.randint(rng_key, (1,), 0, 10)\n",
        "# loss, net, optimizer = update(net, optimizer, (img, label))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Seeding RNG key with seed 42. Use `pax.seed_rng_key` function to avoid this warning.\n",
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jixDaQMym80j",
        "outputId": "f4f8a766-b061-46d8-ab3a-7cb6530a6a64"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "import time \n",
        "start = time.perf_counter()\n",
        "for i in range(10_000):\n",
        "    a, b = jax.tree_flatten((net, optimizer))\n",
        "    net, optimizer = jax.tree_unflatten(b, a)\n",
        "end = time.perf_counter()\n",
        "print(\"Duration:\", end-start)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 31.03061650000018\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J1bqP1em80r",
        "outputId": "8656877b-9d01-4796-afb9-8163f48aefeb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It takes 31.0 seconds to execute 10,000 iterations of `tree_flatten` and `tree_unflatten` for  the tuple `net, optimizer`.\n",
        "\n",
        "This is approximately the extra time (we have to wait) when training a ResNet50 network with an `opax.adam` optimizer for 10,000 iterations."
      ],
      "metadata": {
        "id": "jnZNeI2qpC60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flatten optimizer\n",
        "\n",
        "One easy way to reduce the time is to use the `flatten` mode supported by `opax` optimizers."
      ],
      "metadata": {
        "id": "h1evjLQrqHDe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "optimizer = opax.adam(1e-4)(net.parameters(), flatten=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "CjYQTqugm80s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this mode, the optimizer will automatically flatten the parameters and gradients to a list of leaves instead of dealing with the full tree structure. This reduces the `flatten` and `unflatten` time of the optimizer to almost zero.\n",
        "\n",
        "However, we are no longer able to access the optimizer's pytree objects. \n",
        "Fortunately, we rarely need to access the optimizer's pytree objects, and one can easily convert the flatten list back to the pytree object using `jax.tree_unflatten` function.\n"
      ],
      "metadata": {
        "id": "-SWWQIbYqb6K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "import time \n",
        "start = time.perf_counter()\n",
        "for i in range(10_000):\n",
        "    a, b = jax.tree_flatten((net, optimizer))\n",
        "    net, optimizer = jax.tree_unflatten(b, a)\n",
        "end = time.perf_counter()\n",
        "print(\"Duration:\", end-start)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 7.258846299999277\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo4k-MwHm80t",
        "outputId": "4472ed5e-de15-4754-e441-dc6d195fd601"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "start = time.perf_counter()\n",
        "for i in range(10_000):\n",
        "    a, b = jax.tree_flatten(optimizer)\n",
        "    optimizer = jax.tree_unflatten(b, a)\n",
        "end = time.perf_counter()\n",
        "print(\"Duration:\", end-start)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 0.38130159999855096\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0I79hxtn9qe",
        "outputId": "4ea6c885-1000-414c-c912-0e20861269fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With `flatten=True` we reduce the time to only 7.2 seconds. And the time to `flatten`/`unflatten` the `optimizer` alone is close to zero (0.38 seconds)."
      ],
      "metadata": {
        "id": "PPmiJJ7Ks6W0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-step update function\n",
        "\n",
        "Another solution to reduce the time for `flatten`/`unflatten` is to execute multiple update steps inside a jitted function."
      ],
      "metadata": {
        "id": "i5ttrXP2tRTH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "num_steps = 10\n",
        "\n",
        "@pax.jit\n",
        "def multistep_update(model, optimizer, inputs):\n",
        "    def _step(m_o, i):\n",
        "        m, o, aux = update(*m_o, i)\n",
        "        return (m, o), aux\n",
        "    (model, optimizer), losses = pax.utils.scan(_step, (model, optimizer), inputs)\n",
        "    return model, optimizer, jnp.mean(losses)\n",
        "\n",
        "multistep_img = jax.random.normal(rng_key,  (num_steps, 1, 3, 64, 64))\n",
        "multistep_label = jax.random.randint(rng_key, (num_steps, 1,), 0, 10)\n",
        "# loss, net, optimizer = multistep_update(net, optimizer, (multistep_img, multistep_label))"
      ],
      "outputs": [],
      "metadata": {
        "id": "AVdCuvf0m80v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `multistep_update` function will execute multiple update steps in a single call.\n",
        "If `num_steps=10`, we can reduce the time by a factor of `10`."
      ],
      "metadata": {
        "id": "ybEmPKTQt03C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ".. note::\n",
        "    The practice of executing multiple update steps inside a jitted function is also very useful for TPU. It reduces the communication cost between CPU host and TPU cores, significantly."
      ],
      "metadata": {
        "id": "hDU0JdBjuPiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flatten model"
      ],
      "metadata": {
        "id": "0FzwKNEN57Al"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have reduced the time to `flatten`/`unflatten` the optimizer to almost zero. We can do the same thing for the model too.\n",
        "\n",
        "The idea is simple: we want to move `flatten` and `unflatten` to the inside of the update function."
      ],
      "metadata": {
        "id": "x9zy1WtMvBvF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "from functools import partial\n",
        "\n",
        "@partial(pax.jit, static_argnums=0)\n",
        "def flatten_update(model_def, model_leaves_and_optimizer, inputs):\n",
        "    model_leaves, optimizer = model_leaves_and_optimizer\n",
        "    model = jax.tree_unflatten(model_def, model_leaves)\n",
        "    params = model.parameters()\n",
        "    grads, (loss, model) = pax.grad(loss_fn, has_aux=True)(params, model, inputs)\n",
        "    model, optimizer = pax.apply_gradients(model, optimizer, grads=grads)\n",
        "    return (jax.tree_leaves(model), optimizer), loss"
      ],
      "outputs": [],
      "metadata": {
        "id": "nDvYIwCmvl9r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "net_leaves, net_def = jax.tree_flatten(net)\n",
        "# loss, net_leaves, optimizer = flatten_update(net_def, net_leaves, optimizer, (img, label))"
      ],
      "outputs": [],
      "metadata": {
        "id": "bPtPze3kwTzm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "start = time.perf_counter()\n",
        "for i in range(10_000):\n",
        "    a, b = jax.tree_flatten((net_leaves, optimizer))\n",
        "    (net_leaves, optimizer) = jax.tree_unflatten(b, a)\n",
        "end = time.perf_counter()\n",
        "print(\"Duration:\", end-start)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 0.6903635999988182\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XTe1Rq8m806",
        "outputId": "9d46f02f-e296-4e70-e4a8-02c85c5aa56c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now only wait an extra time of `0.69` seconds when training a ResNet50 for 10,000 steps.\n",
        "\n",
        "However, we have to manually recreate the model from its leaves and tree_def."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "net = jax.tree_unflatten(net_def, net_leaves)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pax provides a similar functionality with `pax.nn.FlattenModule`. It creates a new module with all parameters and states are flatten."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "flatten_net = pax.flatten_module(net)\n",
        "\n",
        "# to recreate the original module\n",
        "net = flatten_net.unflatten() "
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The functionality of `flatten_module` is limited as it is designed for performance purpose only."
      ],
      "metadata": {}
    }
  ]
}